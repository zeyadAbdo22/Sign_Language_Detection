Sign Language Detection is a machine learning-based project aimed at recognizing American Sign Language (ASL) alphabet gestures using computer vision. The system leverages a Convolutional Neural Network (CNN) model trained on a dataset of ASL hand signs to detect and interpret gestures in real time via webcam or through uploaded images.

This project provides an accessible way for users to bridge the communication gap with individuals who rely on sign language, by converting visual hand signs into corresponding letters of the alphabet. With support for the full ASL alphabet, along with 'space' and 'nothing' gestures, the system facilitates seamless interaction between signers and non-signers.

The real-time detection capabilities make the application suitable for a variety of use cases, such as educational tools for learning ASL or assistive technology for communication.

Key technologies used in this project include:

TensorFlow/Keras: For building and training the CNN model.
OpenCV: For real-time image processing and webcam integration.
Flask: For providing a user-friendly web interface.
Whether you're looking to detect sign language in real time or analyze hand gestures from pre-captured images, this project offers a practical solution to enhance communication through the power of machine learning.
